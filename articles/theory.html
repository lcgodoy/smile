<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>4. Method • smile</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/Roboto-0.4.9/font.css" rel="stylesheet">
<link href="../deps/JetBrains_Mono-0.4.9/font.css" rel="stylesheet">
<link href="../deps/Roboto_Slab-0.4.9/font.css" rel="stylesheet">
<link href="../deps/font-awesome-6.4.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.4.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="4. Method">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-primary" data-bs-theme="dark" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">smile</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">1.0.5</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../index.html"><span class="fa fa-home fa-lg"></span></a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-vignettes" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true"><span class="fa fa-book fa-lg"></span> Vignettes</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-vignettes">
<li><a class="dropdown-item" href="../articles/sf-to-spm.html">1. Converting sf objects to spm</a></li>
    <li><a class="dropdown-item" href="../articles/fit-and-pred.html">2. Fitting models and making predictions</a></li>
    <li><a class="dropdown-item" href="../articles/sai.html">3. Areal Interpolation</a></li>
    <li><a class="dropdown-item" href="../articles/theory.html">4. Method</a></li>
    <li><a class="dropdown-item" href="../articles/sp-cov-functions.html">5. Spatial covariance functions</a></li>
  </ul>
</li>
<li class="nav-item"><a class="nav-link" href="../reference/index.html"><span class="fa fa-search fa-lg"></span> Reference</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><a class="external-link nav-link" href="https://lcgodoy.me"><span class="fa fa-globl"></span></a></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/lcgodoy"><span class="fa fa-github"></span></a></li>
<li class="nav-item"><a class="external-link nav-link" href="https://www.linkedin.com/in/godoy-lucas/"><span class="fa fa-linkedin"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.png" class="logo" alt=""><h1>4. Method</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/lcgodoy/smile/blob/main/vignettes/theory.Rmd" class="external-link"><code>vignettes/theory.Rmd</code></a></small>
      <div class="d-none name"><code>theory.Rmd</code></div>
    </div>

    
    
<div class="section level3">
<h3 id="general-setup">General setup<a class="anchor" aria-label="anchor" href="#general-setup"></a>
</h3>
<p>Let us set up the notations first. Suppose a there exists a partition
of a region <span class="math inline">${\rm D} \in {\cal R}^2$</span>
(e.g., a city). This partition is denoted by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>A</mi><mi>i</mi></msub><annotation encoding="application/x-tex">A_i</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>…</mi><mo>,</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">i = 1, \ldots, n</annotation></semantics></math>.
Moreover, there exists another partition of the same city, denoted
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>B</mi><mi>j</mi></msub><annotation encoding="application/x-tex">B_j</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>…</mi><mo>,</mo><mi>m</mi></mrow><annotation encoding="application/x-tex">j = 1, \ldots, m</annotation></semantics></math>.
These partitions can be seen as two different administrative divisions
within the same city. It is common for different government agencies to
release data for different divisions of a same city, country, or
state.</p>
</div>
<div class="section level3">
<h3 id="model-based-approach">Model-based approach<a class="anchor" aria-label="anchor" href="#model-based-approach"></a>
</h3>
<p>Assume we observe a random variable
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo>⋅</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">Y(\cdot)</annotation></semantics></math>
at each region
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>A</mi><mi>i</mi></msub><annotation encoding="application/x-tex">A_i</annotation></semantics></math>
and we are interested in predict/estimate this variable in each of the
regions
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>B</mi><mi>j</mi></msub><annotation encoding="application/x-tex">B_j</annotation></semantics></math>.
Now suppose the random variable
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo>⋅</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">Y(\cdot)</annotation></semantics></math>
varies continuously over <span class="math inline">${\rm D}$</span> and
is defined as follows <span class="math display">$$
Y(\mathbf{s}) = \mu + S(\mathbf{s}) + \varepsilon(\mathbf{s}), \,
\mathbf{s} \in
{\rm D} \subset {\cal R}^2.
$$</span> where <span class="math display">$$
S(\cdot) \sim {\rm GP}(0, \sigma^2 \rho(\cdot; \, \phi, \kappa)) \;
\text{ and } \;
\varepsilon(\cdot) \overset{{\rm i.i.d.}}{\sim} {\rm N}(0, \sigma^2
\rho(\cdot;
\, \phi, \kappa)),
$$</span> with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ε</mi><annotation encoding="application/x-tex">\varepsilon</annotation></semantics></math>
independent of each other. For now, let’s make the unrealistic
assumption that all those parameters are known. Then, our assumption is
that the observed data is as follows <span class="math display">$$\begin{align*}
Y(A_i) &amp; = \frac{1}{\lvert A_i \rvert} \int_{A_i} Y(\mathbf{s}) \,
{\rm d}
\mathbf{s} \\
&amp; = \frac{1}{\lvert A_i \rvert} \int_{A_i} [\mu + S(\mathbf{s}) +
\varepsilon(\mathbf{s})] \, {\rm d} \mathbf{s} \\
&amp; = \mu + \frac{1}{\lvert A_i \rvert} \int_{A_i} S(\mathbf{s}) {\rm
d}
\mathbf{s} + \frac{1}{\lvert A_i \rvert} \int_{A_i}
\varepsilon(\mathbf{s}) {\rm
d} \mathbf{s},
\end{align*}$$</span> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">|</mo><mo>⋅</mo><mo stretchy="false" form="postfix">|</mo></mrow><annotation encoding="application/x-tex">\lvert \cdot \rvert</annotation></semantics></math>
returns the area of a polygon. Furthermore, it can be shown that (using
Fubini’s Theorem and some algebraic manipulation) <span class="math display">$$
{\rm Cov}(Y(A_i), Y(A_j)) = \frac{\sigma^2}{\lvert A_i \rvert \lvert A_j
\rvert}
\int_{A_i \times A_j} \rho( \lVert \mathbf{s} - \mathbf{s}' \rVert; \,
\phi,
\kappa ) \, {\rm d} \mathbf{s} \, {\rm d} \mathbf{s}' + \mathbf{I}(i =
j)
\frac{\tau}{\lvert A_i \rvert},
$$</span> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ρ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo>⋅</mo><mo>;</mo><mspace width="0.167em"></mspace><mi>ϕ</mi><mo>,</mo><mi>κ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\rho(\cdot ; \, \phi, \kappa)</annotation></semantics></math>
is a positive definite correlation function. Now, let <span class="math inline">${\rm R}_{\kappa}(\phi)$</span> be a correlation
matrix such that <span class="math display">$$
{\rm R}_{\kappa}(\phi)_{ij} = \frac{1}{\lvert A_i \rvert \lvert A_j
\rvert}
\int_{A_i \times A_j} \rho( \lVert \mathbf{s} - \mathbf{s}' \rVert; \,
\phi,
\kappa ) \, {\rm d} \mathbf{s} \, {\rm d} \mathbf{s}',
$$</span> thus, <span class="math display">$$
Y(A_1, \cdots, A_n) \sim {\rm N}( \mu \mathbf{1}_n, \sigma^2 {\rm
R}_{\kappa}(\phi) + \tau {\rm diag}(\lvert A_1 \rvert^{-1}, \ldots,
\lvert A_1
\rvert^{-1})).
$$</span> Then, if we assume
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>Y</mi><mi>⊤</mi></msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>A</mi><mn>1</mn></msub><mo>,</mo><mi>⋯</mi><mo>,</mo><msub><mi>A</mi><mi>n</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><msup><mi>Y</mi><mi>⊤</mi></msup><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>B</mi><mn>1</mn></msub><mo>,</mo><mi>⋯</mi><mo>,</mo><msub><mi>A</mi><mi>m</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>⊤</mi></msup><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(Y^{\top}(A_1, \cdots, A_n), Y^{\top}(B_1, \cdots,
A_m)^{\top})</annotation></semantics></math> to be jointly normal, we
use can the conditional mean of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>Y</mi><mi>⊤</mi></msup><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>B</mi><mn>1</mn></msub><mo>,</mo><mi>⋯</mi><mo>,</mo><msub><mi>A</mi><mi>m</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>⊤</mi></msup></mrow><annotation encoding="application/x-tex">Y^{\top}(B_1, \cdots, A_m)^{\top}</annotation></semantics></math>
given
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>Y</mi><mi>⊤</mi></msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>A</mi><mn>1</mn></msub><mo>,</mo><mi>⋯</mi><mo>,</mo><msub><mi>A</mi><mi>n</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">Y^{\top}(A_1, \cdots, A_n)</annotation></semantics></math>
to estimate the observed random variable in the partition
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>B</mi><mn>1</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>B</mi><mi>m</mi></msub></mrow><annotation encoding="application/x-tex">B_1, \ldots, B_m</annotation></semantics></math>.</p>
<hr>
<p>Now, suppose the parameters
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝛉</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>μ</mi><mo>,</mo><msup><mi>σ</mi><mn>2</mn></msup><mo>,</mo><mi>ϕ</mi><mo>,</mo><mi>τ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\boldsymbol{\theta} = (\mu, \sigma^2, \phi, \tau)</annotation></semantics></math>
are unknown. The Likelihood of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>A</mi><mn>1</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>A</mi><mi>n</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">Y(A_1, \ldots, A_n)</annotation></semantics></math>
can still be computed.</p>
<p>In particular, if we use the parametrization
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ν</mi><mo>=</mo><mi>τ</mi><mi>/</mi><msup><mi>σ</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">\nu = \tau / \sigma^2</annotation></semantics></math>,
we have closed form for the Maximum Likelihood estimators both for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>μ</mi><annotation encoding="application/x-tex">\mu</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>σ</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\sigma^2</annotation></semantics></math>.
Thus, we can optimize the profile likelihood for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ϕ</mi><annotation encoding="application/x-tex">\phi</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ν</mi><annotation encoding="application/x-tex">\nu</annotation></semantics></math>
numerically. Then, we resort on conditional Normal properties again to
compute the predictions in a new different set of regions.</p>
</div>
<div class="section level3">
<h3 id="areal-interpolation-ai">Areal Interpolation (AI)<a class="anchor" aria-label="anchor" href="#areal-interpolation-ai"></a>
</h3>
<p>Areal interpolation is a nonparametric approach that interpolates
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>A</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">Y(A_i)</annotation></semantics></math>’s
to construct
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>B</mi><mi>j</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">Y(B_j)</annotation></semantics></math>’s.
Define an
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">m \times n</annotation></semantics></math>
matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝐖</mi><mo>=</mo><mo stretchy="false" form="prefix">{</mo><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">\mathbf{W} = \{ w_{ij} \}</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">w_{ij}</annotation></semantics></math>
is the weight associated with the polygon
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>A</mi><mi>i</mi></msub><annotation encoding="application/x-tex">A_i</annotation></semantics></math>
in constructing
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>B</mi><mi>j</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">Y(B_j)</annotation></semantics></math>.
The weights are
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mo stretchy="false" form="prefix">|</mo><msub><mi>A</mi><mi>i</mi></msub><mo>∩</mo><msub><mi>B</mi><mi>j</mi></msub><mo stretchy="false" form="postfix">|</mo><mi>/</mi><mo stretchy="false" form="prefix">|</mo><msub><mi>B</mi><mi>j</mi></msub><mo stretchy="false" form="postfix">|</mo></mrow><annotation encoding="application/x-tex">w_{ij} = \lvert A_i \cap B_j \rvert / \lvert B_j \rvert</annotation></semantics></math><span class="citation">(Goodchild and Lam 1980; Gotway and Young
2002)</span>. The interpolation for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>Y</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>B</mi><mn>1</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>B</mi><mi>m</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\hat Y(B_1, \ldots, B_m)</annotation></semantics></math>
is constructed as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>Y</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>B</mi><mn>1</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>B</mi><mi>m</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>𝐖</mi><mi>Y</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>A</mi><mn>1</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>A</mi><mi>n</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation}
  \label{eq:np-est}
  \hat{Y}(B_1, \ldots, B_m) = \mathbf{W}
  Y(A_1, \ldots, A_n).
\end{equation}</annotation></semantics></math> The expectation and
variance of the predictor are, respectively, <span class="math display">$$
  {\rm E}[\hat{Y}(B_1, \ldots, B_m)] = \mathbf{W}
  {\rm E}[Y(A_1, \ldots, A_n)]
$$</span> and
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">Var</mtext><mrow><mo stretchy="true" form="prefix">[</mo><mover><mi>Y</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>B</mi><mn>1</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>B</mi><mi>m</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mi>𝐖</mi><mtext mathvariant="normal">Var</mtext><mrow><mo stretchy="true" form="prefix">[</mo><mi>Y</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>A</mi><mn>1</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>A</mi><mi>n</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><msup><mi>𝐖</mi><mi>⊤</mi></msup><mi>.</mi></mrow><annotation encoding="application/x-tex">\begin{equation}
  \label{eq:np-matcov}
  \textrm{Var}[\hat{Y}(B_1, \ldots, B_m)] = \mathbf{W}
  \textrm{Var}[Y(A_1, \ldots, A_n)] \mathbf{W}^{\top}.
\end{equation}</annotation></semantics></math> In practice, the
covariance matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">Var</mtext><mrow><mo stretchy="true" form="prefix">[</mo><mi>Y</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>A</mi><mn>1</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>A</mi><mi>n</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">\textrm{Var}[Y(A_1, \ldots, A_n)]</annotation></semantics></math>
is unknown and, consequently needs to be estimated.</p>
<p>The variance each predictor
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">Var</mtext><mrow><mo stretchy="true" form="prefix">[</mo><mover><mi>Y</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>B</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">\text{Var}[\hat Y(B_i)]</annotation></semantics></math>
is needed as an uncertainty measure. It relies on both the variances of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>A</mi><mi>j</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">Y(A_j)</annotation></semantics></math>’s
and their covariances:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">Var</mtext><mrow><mo stretchy="true" form="prefix">[</mo><mover><mi>Y</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>B</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msubsup><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow><mn>2</mn></msubsup><mtext mathvariant="normal">Var</mtext><mrow><mo stretchy="true" form="prefix">[</mo><mi>Y</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>A</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mo>+</mo><mn>2</mn><munder><mo>∑</mo><mrow><mi>l</mi><mo>≠</mo><mi>i</mi></mrow></munder><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><msub><mi>w</mi><mrow><mi>i</mi><mi>l</mi></mrow></msub><mtext mathvariant="normal">Cov</mtext><mrow><mo stretchy="true" form="prefix">[</mo><mi>Y</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>A</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>Y</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>A</mi><mi>l</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mi>.</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align}
  \label{eq:np-single-var}
  \textrm{Var}[\hat{Y}(B_i)] 
  = \sum_{i = 1}^n w^2_{ij} \textrm{Var} \left [ Y(A_i) \right ] + 2 \sum_{l
  \neq i} w_{ij} w_{il} \textrm{Cov} \left[ Y(A_i), Y(A_l) \right].
\end{align}</annotation></semantics></math> The variances are often
observed in survey data, but the covariances are not. For practical
purpose, we propose an approximation for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">Cov</mtext><mrow><mo stretchy="true" form="prefix">[</mo><mi>Y</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>A</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>Y</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>A</mi><mi>l</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">\textrm{Cov}[ Y(A_i), Y(A_l)]</annotation></semantics></math>
based on Moran’s I, a global spatial autocorrelation. Specifically, let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>ρ</mi><mi>I</mi></msub><annotation encoding="application/x-tex">\rho_I</annotation></semantics></math>
be the Moran’s I calculated with a weight matrix constructed with
first-degree neighbors. That is,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>ρ</mi><mi>I</mi></msub><annotation encoding="application/x-tex">\rho_I</annotation></semantics></math>
is the average of the pairwise correlation for all neighboring pairs.
For regions
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>A</mi><mi>i</mi></msub><annotation encoding="application/x-tex">A_i</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>A</mi><mi>l</mi></msub><annotation encoding="application/x-tex">A_l</annotation></semantics></math>,
if they are neighbors of each other, our approximation is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">Cov</mtext><mrow><mo stretchy="true" form="prefix">[</mo><mi>Y</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>A</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>Y</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>A</mi><mi>l</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><msub><mi>ρ</mi><mi>I</mi></msub><msqrt><mrow><mtext mathvariant="normal">Var</mtext><mrow><mo stretchy="true" form="prefix">[</mo><mi>Y</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>A</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mtext mathvariant="normal">Var</mtext><mrow><mo stretchy="true" form="prefix">[</mo><mi>Y</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>A</mi><mi>l</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow></mrow></msqrt><mi>.</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align}
  \label{eq:cova}
  \textrm{Cov} \left[ Y(A_i), Y(A_l) \right]
  = \rho_I \sqrt{\text{Var}[Y(A_i)]  \text{Var}[Y(A_l)]}.
\end{align}</annotation></semantics></math> The covariance between
non-neighboring
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>A</mi><mi>i</mi></msub><annotation encoding="application/x-tex">A_i</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>A</mi><mi>l</mi></msub><annotation encoding="application/x-tex">A_l</annotation></semantics></math>
is discarded. The final uncertainty approximation for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">Var</mtext><mrow><mo stretchy="true" form="prefix">[</mo><mover><mi>Y</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>B</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">\textrm{Var}[\hat{Y}(B_i)]</annotation></semantics></math>
will be an underestimate. Alternatively, we can derive, at least, an
upper bound for the variance of the estimates by using a simple
application from the Cauchy–Schwartz inequality, in which case,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>ρ</mi><mi>I</mi></msub><annotation encoding="application/x-tex">\rho_I</annotation></semantics></math>
is replaced with~1.</p>
</div>
<div class="section level2">
<h2 class="unnumbered" id="reference">Reference<a class="anchor" aria-label="anchor" href="#reference"></a>
</h2>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-goodchild1980areal" class="csl-entry">
Goodchild, Michael F, and Nina Siu-Ngan Lam. 1980. <span>“Areal
Interpolation: <span>A</span> Variant of the Traditional Spatial
Problem.”</span> <em>Geo-Processing</em> 1: 279–312.
</div>
<div id="ref-gotway2002combining" class="csl-entry">
Gotway, Carol A, and Linda J Young. 2002. <span>“Combining Incompatible
Spatial Data.”</span> <em>Journal of the American Statistical
Association</em> 97 (458): 632–48.
</div>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Powered by <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a>.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Lucas da Cunha Godoy. Provided without any warranty.</p>
</div>

    </footer>
</div>





  </body>
</html>
